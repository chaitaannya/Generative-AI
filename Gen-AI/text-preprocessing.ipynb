{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3f3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d3400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\chait\\OneDrive\\Documents\\Generative-Ai\\Generative-AI\\Gen-AI\\IMDB Dataset.csv\"\n",
    "df=pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95656464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Daniel Day-Lewis is the most versatile actor a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>My guess would be this was originally going to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Well, I like to watch bad horror B-Movies, cau...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>This IS the worst movie I have ever seen, as w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I have been a Mario fan for as long as I can r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   One of the other reviewers has mentioned that ...  positive\n",
       "1   A wonderful little production. <br /><br />The...  positive\n",
       "2   I thought this was a wonderful way to spend ti...  positive\n",
       "3   Basically there's a family where a little boy ...  negative\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                ...       ...\n",
       "95  Daniel Day-Lewis is the most versatile actor a...  positive\n",
       "96  My guess would be this was originally going to...  negative\n",
       "97  Well, I like to watch bad horror B-Movies, cau...  negative\n",
       "98  This IS the worst movie I have ever seen, as w...  negative\n",
       "99  I have been a Mario fan for as long as I can r...  positive\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa8e8f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ad0583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab540e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d86d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()\n",
    "\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f987eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f599acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of html tags\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    clean=re.compile('<.*?>')\n",
    "    return re.sub(clean,'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "790b9409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample text after removing HTML tags.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"<p>This is a <b>sample</b> text after removing <a href='#'>HTML tags</a>.</p>\"\n",
    "cleaned_text = remove_html_tags(text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3355cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac0b318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(pattern,'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64f0817b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my linked in profile '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is my linked in profile https://www.linkedin.com/in/chaitanyasai07/\"\n",
    "cleaned_text = remove_url(text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c7f61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed urls from the reviews\n",
    "df['review'] = df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2158a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,time\n",
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5708f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5043b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to remove punctuation: 2.0503997802734375 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is  a sample text with removed punctuation'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this, is , a sample. text with removed! punctuation\"\n",
    "start=time.time()\n",
    "cleaned_text = remove_punctuation(text)\n",
    "end=time.time()-start\n",
    "print(f\"Time taken to remove punctuation: {end*50000} seconds\")\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e6616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encouraged by the positive comments about this film on here i was looking forward to watching this film bad mistake ive seen 950 films and this is truly one of the worst of them  its awful in almost every way editing pacing storyline acting soundtrack the films only song  a lame country tune  is played no less than four times the film looks cheap and nasty and is boring in the extreme rarely have i been so happy to see the end credits of a film the only thing that prevents me giving this a 1score is harvey keitel  while this is far from his best performance he at least seems to be making a bit of an effort one for keitel obsessives only'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].apply(remove_punctuation)\n",
    "df['review'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6673b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words={\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"4\": \"for\", \n",
    "    \"ur\": \"your\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"afaik\":\"as far as i know\",\n",
    "    \"ikr\":\"i know right\",\n",
    "    \"faq\":\"frequently asked question\",\n",
    "    \"fyi\":\"for your information\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b20fb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_convertion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if(w.lower() in chat_words):\n",
    "            new_text.append(chat_words[w.lower()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00809ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by the way , you are going to do this for your project'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_convertion('btw , u r going to do this 4 ur project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82a7e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60a5a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the swellings of this sentence is changes after using textbolbb\n"
     ]
    }
   ],
   "source": [
    "#to correct spelling mistakes\n",
    "from textblob import TextBlob\n",
    "\n",
    "incorrect_text = \"the spallings of thuis santance is changus after usjng textbolbb\"\n",
    "blob = TextBlob(incorrect_text)\n",
    "\n",
    "# Option 1: just print the corrected TextBlob\n",
    "print(blob.correct())\n",
    "\n",
    "# Option 2: get a plain string\n",
    "corrected = str(blob.correct())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "269d20ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords removal\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b5cfd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285447f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49d4d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    newtext=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            newtext.append('')\n",
    "        else:\n",
    "            newtext.append(word)\n",
    "    x=newtext[:]\n",
    "    newtext.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aab9312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   sample text   stopwords like   '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('this is a sample text with some stopwords like the and is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54cdff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8659ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one    reviewers  mentioned   watching  1 oz e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production  filming techniqu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought    wonderful way  spend time    hot s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres  family   little boy jake thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love   time  money   visually s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably  alltime favorite movie  story  selfl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure would like  see  resurrection    dated s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show   amazing fresh innovative idea   70s   ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged   positive comments   film     look...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like original gut wrenching laughter   like ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phil  alien  one   quirky films   humour  base...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>saw  movie     12   came   recall  scariest s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>im   big fan  bolls work     many   enjoyed  ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cast played shakespeareshakespeare losti appr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fantastic movie  three prisoners  become fam...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kind  drawn    erotic scenes   realize   one  ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>films  simply    remade   one           bad f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>movie made   one   top 10  awful movies horri...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>remember  filmit   first film   watched   cin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>awful film  must      real stinkers   nominat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   one    reviewers  mentioned   watching  1 oz e...  positive\n",
       "1    wonderful little production  filming techniqu...  positive\n",
       "2    thought    wonderful way  spend time    hot s...  positive\n",
       "3   basically theres  family   little boy jake thi...  negative\n",
       "4   petter matteis love   time  money   visually s...  positive\n",
       "5   probably  alltime favorite movie  story  selfl...  positive\n",
       "6    sure would like  see  resurrection    dated s...  positive\n",
       "7    show   amazing fresh innovative idea   70s   ...  negative\n",
       "8   encouraged   positive comments   film     look...  negative\n",
       "9     like original gut wrenching laughter   like ...  positive\n",
       "10  phil  alien  one   quirky films   humour  base...  negative\n",
       "11   saw  movie     12   came   recall  scariest s...  negative\n",
       "12   im   big fan  bolls work     many   enjoyed  ...  negative\n",
       "13   cast played shakespeareshakespeare losti appr...  negative\n",
       "14    fantastic movie  three prisoners  become fam...  positive\n",
       "15  kind  drawn    erotic scenes   realize   one  ...  negative\n",
       "16   films  simply    remade   one           bad f...  positive\n",
       "17   movie made   one   top 10  awful movies horri...  negative\n",
       "18   remember  filmit   first film   watched   cin...  positive\n",
       "19   awful film  must      real stinkers   nominat...  negative"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf1b5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emojis removal\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e1b132f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample text with emojis  and some more text.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis('This is a sample text with emojis ðŸ˜ŠðŸš€ and some more text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40f94b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(\"lol ðŸ˜‚ðŸ˜‚ðŸ˜‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5d55315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e094b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":face_with_tears_of_joy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the movie of :fire:'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('ðŸ˜‚'))\n",
    "emoji.demojize('the movie of ðŸ”¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af793e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f315705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "sent=\"hello, how are you?\"\n",
    "tokens=word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c405887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f259091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "df=pd.DataFrame({'text':[\"I am running\",\"He was running\",\"They are playing\",\"She is swimming\"],'output':[0,1,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9c34430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am running</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He was running</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They are playing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She is swimming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text  output\n",
       "0      I am running       0\n",
       "1    He was running       1\n",
       "2  They are playing       0\n",
       "3   She is swimming       1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c99ee850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "341495c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd9535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am': 0, 'running': 5, 'he': 2, 'was': 9, 'they': 8, 'are': 1, 'playing': 4, 'she': 6, 'is': 3, 'swimming': 7}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9abd0aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ab70b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3214776f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['she is running']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "666e9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=bow.toarray()\n",
    "y=df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "112be7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "cv=CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a8076b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdbb45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am running': 0, 'he was': 2, 'was running': 6, 'they are': 5, 'are playing': 1, 'she is': 4, 'is swimming': 3}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "970a3790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb2a507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['they are swimming']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b5db922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above we considered two-grams(words) so it is called are bigrams same way we can form trigrams, four-grams etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d0ebf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF term frequency-inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c88a88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=tfidf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5523223f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78528828, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.6191303 , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.61761437, 0.        , 0.        ,\n",
       "        0.48693426, 0.        , 0.        , 0.        , 0.61761437],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.57735027, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
       "        0.        , 0.57735027, 0.57735027, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a29ed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78528828 0.         0.         0.         0.         0.6191303\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.61761437 0.         0.         0.48693426\n",
      "  0.         0.         0.         0.61761437]\n",
      " [0.         0.57735027 0.         0.         0.57735027 0.\n",
      "  0.         0.         0.57735027 0.        ]\n",
      " [0.         0.         0.         0.57735027 0.         0.\n",
      "  0.57735027 0.57735027 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(arr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea7183dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting features from game of thrones book dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
